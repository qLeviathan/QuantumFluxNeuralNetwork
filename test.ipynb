{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c81c8-45e9-4d24-8d87-6ef4c1d7feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Refactored Quantum-Geometric Code WITHOUT Gradients or Backprop\n",
    "---------------------------------------------------------------\n",
    "This script:\n",
    "  1) Demonstrates radial diffusion PDE updates (Heun-Euler).\n",
    "  2) Maintains embeddings for tokens, updates them iteratively based on\n",
    "     a negative-distance \"score\" (no gradient, just numeric rules).\n",
    "  3) PDE residual acts as a measure of how \"aligned\" or \"relaxed\" the wavefunction is.\n",
    "  4) Negative distance among tokens measures how \"close\" or \"far\" tokens are in embedding space.\n",
    "  5) We apply dynamic dt updates or embedding updates using these scores, \n",
    "     with no .backward(), no optimizer, no autograd.\n",
    "\n",
    "Usage:\n",
    "   python quantum_no_grad.py\n",
    "\n",
    "Date: 2025-02-09\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.profiler\n",
    "\n",
    "###############################################################################\n",
    "# I. Utility: Device, Precision (No Grad)\n",
    "###############################################################################\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Return CUDA device if available, else CPU.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "# II. Radial Diffusion PDE: (No Potential), Heun Step\n",
    "###############################################################################\n",
    "\n",
    "def radial_diffusion_rhs(u: torch.Tensor, r_grid: torch.Tensor, alpha: float):\n",
    "    \"\"\"\n",
    "    PDE: ∂u/∂t = alpha * [1/r d/dr ( r d/dr u ) ], potential=0 by default.\n",
    "    No gradients: we do a direct finite difference numeric approach.\n",
    "    \"\"\"\n",
    "    du_dt = torch.zeros_like(u)\n",
    "    dr = r_grid[1] - r_grid[0]\n",
    "    n = r_grid.shape[0]\n",
    "\n",
    "    for i in range(1, n-1):\n",
    "        r = r_grid[i]\n",
    "        d_plus  = (u[i+1] - u[i]) / dr\n",
    "        d_minus = (u[i] - u[i-1]) / dr\n",
    "        flux_plus  = r * d_plus\n",
    "        flux_minus = r * d_minus\n",
    "        du_dt[i] = alpha*(1.0/r)*(flux_plus - flux_minus)/dr\n",
    "\n",
    "    return du_dt\n",
    "\n",
    "def heun_euler_step(u: torch.Tensor,\n",
    "                    r_grid: torch.Tensor,\n",
    "                    alpha: float,\n",
    "                    dt: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Classic Heun-Euler step (no autograd):\n",
    "      k1 = radial_diffusion_rhs(u)\n",
    "      k2 = radial_diffusion_rhs(u + dt*k1)\n",
    "      return u + dt/2*(k1 + k2)\n",
    "    \"\"\"\n",
    "    k1 = radial_diffusion_rhs(u, r_grid, alpha)\n",
    "    u_plus = u + dt*k1\n",
    "    k2 = radial_diffusion_rhs(u_plus, r_grid, alpha)\n",
    "    return u + 0.5*dt*(k1 + k2)\n",
    "\n",
    "###############################################################################\n",
    "# III. Negative Distance & Embedding Updates\n",
    "###############################################################################\n",
    "\n",
    "def negative_distance_matrix(embs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    M[i,j] = -||embs[i] - embs[j]|| (no gradient).\n",
    "    \"\"\"\n",
    "    D_ = embs.shape[0]\n",
    "    M = torch.zeros(D_, D_, device=embs.device, dtype=embs.dtype)\n",
    "    for i in range(D_):\n",
    "        for j in range(D_):\n",
    "            dist = (embs[i] - embs[j]).norm()\n",
    "            M[i, j] = -dist\n",
    "    return M\n",
    "\n",
    "def update_embeddings(embs: torch.Tensor,\n",
    "                      dt: float,\n",
    "                      dist_scale: float = 0.1\n",
    "                      ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Example numeric update rule for embeddings:\n",
    "      - Compute negative distance matrix => mean_negdist\n",
    "      - Move embeddings to get \"closer\" or \"farther\" by some numeric scheme\n",
    "        (no gradient, just direct manipulations).\n",
    "\n",
    "    Here, we do a simple approach: \n",
    "      1) M = negative_distance_matrix(embs) => mean_negdist\n",
    "      2) embs += dt * dist_scale * mean_negdist * direction(???)\n",
    "\n",
    "    Since we have no gradient, we define a simple 'collapse' step:\n",
    "      For each embs[i], shift it slightly toward the average of all other vectors.\n",
    "      The magnitude of this shift is dt * dist_scale * mean_negdist.\n",
    "\n",
    "    This is a toy example of \"relaxation\" to reduce distance among tokens.\n",
    "    \"\"\"\n",
    "    M = negative_distance_matrix(embs)\n",
    "    mean_negdist = M.mean().item()  # scalar\n",
    "    D_ = embs.shape[0]\n",
    "    # Compute \"average embedding\" to drive them closer\n",
    "    avg_vec = embs.mean(dim=0, keepdim=True)  # shape [1, E]\n",
    "\n",
    "    # We'll define \"shift\" = (avg_vec - embs[i]) => pulling each embedding\n",
    "    # slightly toward the center. Magnitude scaled by (dt * dist_scale * mean_negdist).\n",
    "    shift_mag = dt * dist_scale * mean_negdist\n",
    "\n",
    "    # shift all\n",
    "    embs_new = embs + shift_mag*(avg_vec - embs)\n",
    "    return embs_new\n",
    "\n",
    "###############################################################################\n",
    "# IV. PDE Residual as a Score, Dynamic dt\n",
    "###############################################################################\n",
    "\n",
    "def pde_residual(u: torch.Tensor, r_grid: torch.Tensor, alpha: float):\n",
    "    \"\"\"\n",
    "    PDE residual => radial_diffusion_rhs(u). If we want \"steady-state,\" \n",
    "    we want residual ~ 0. We'll treat the mean^2 of this as a 'score'.\n",
    "    \"\"\"\n",
    "    res = radial_diffusion_rhs(u, r_grid, alpha)\n",
    "    return (res**2).mean().item()\n",
    "\n",
    "def dynamic_dt_update(dt: float,\n",
    "                      pde_score: float,\n",
    "                      negdist_score: float,\n",
    "                      dt_scale: float = 0.1):\n",
    "    \"\"\"\n",
    "    Combine PDE score and negative distance to modify dt:\n",
    "      dt_new = dt * (1 - dt_scale*(pde_score + negdist_score))\n",
    "    or any approach you want. We'll do a simple approach here.\n",
    "\n",
    "    If pde_score or negdist_score are large => dt shrinks => more fine steps.\n",
    "    If they are small => dt grows => bigger steps.\n",
    "    \"\"\"\n",
    "    adjust = pde_score + negdist_score\n",
    "    dt_new = dt * (1.0 - dt_scale*adjust)\n",
    "    if dt_new < 1e-6:\n",
    "        dt_new = 1e-6\n",
    "    return dt_new\n",
    "\n",
    "###############################################################################\n",
    "# V. Synthetic Dataset (No Grad) & Iterative \"Training\"\n",
    "###############################################################################\n",
    "\n",
    "class SyntheticTokenDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Large dataset of random tokens (batch-based). We'll just iterate \n",
    "    and do PDE + embedding updates for each batch. No gradient usage.\n",
    "    \"\"\"\n",
    "    def __init__(self, total_samples=1000, seq_length=32, vocab_size=10000):\n",
    "        super().__init__()\n",
    "        self.total_samples = total_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return random tokens\n",
    "        return torch.randint(low=0, high=self.vocab_size, size=(self.seq_length,))\n",
    "\n",
    "###############################################################################\n",
    "# VI. Helper Functions: Sentence -> Embeddings\n",
    "###############################################################################\n",
    "\n",
    "def polar_to_cartesian(r, theta):\n",
    "    rx = r*math.cos(theta)\n",
    "    ry = r*math.sin(theta)\n",
    "    return rx, ry\n",
    "\n",
    "def sentence_to_embeddings(sentence, N, device, dtype):\n",
    "    \"\"\"\n",
    "    Like before: \n",
    "    r_i = 0.3 + 0.6*(i/(D-1))\n",
    "    theta_i = 2π*(i/D)\n",
    "    (rx, ry) => first 2 dims => leftover N-2=0\n",
    "    \"\"\"\n",
    "    D_ = len(sentence)\n",
    "    embs = torch.zeros((D_, N), device=device, dtype=dtype)\n",
    "    for i in range(D_):\n",
    "        if D_>1:\n",
    "            frac = i/float(D_-1)\n",
    "        else:\n",
    "            frac=0.0\n",
    "        r_i = 0.3 + 0.6*frac\n",
    "        theta_i = 2.0*math.pi*(i/float(D_)) if D_>0 else 0.0\n",
    "        rx, ry = polar_to_cartesian(r_i, theta_i)\n",
    "        embs[i,0] = rx\n",
    "        embs[i,1] = ry\n",
    "    return embs\n",
    "\n",
    "###############################################################################\n",
    "# VII. Main No-Grad Loop\n",
    "###############################################################################\n",
    "\n",
    "def main():\n",
    "    device = get_device()\n",
    "    print(\"[INFO] Device:\", device)\n",
    "\n",
    "    # Basic numeric config\n",
    "    dtype = torch.float32\n",
    "    # PDE alpha = 2 => if we interpret N=4 => alpha= N/2 => just pick alpha=2\n",
    "    alpha = 2.0\n",
    "\n",
    "    # Radial grid\n",
    "    n_r = 50\n",
    "    r_grid = torch.linspace(0,1, steps=n_r, device=device, dtype=dtype)\n",
    "\n",
    "    # PDE wavefunction\n",
    "    # We'll store it in a tensor. Initialization => Gaussian around r=0.5\n",
    "    # no gradients => direct numeric updates\n",
    "    u = torch.exp(-((r_grid-0.5)**2)/(2*0.01))\n",
    "\n",
    "    # dt init\n",
    "    dt = 0.05  # a fixed start or from inverse beta if you prefer\n",
    "\n",
    "    # Example sentence -> embeddings\n",
    "    sentence = [\"I\",\"like\",\"quantum\",\"mechanics\",\"with\",\"pizza\"]\n",
    "    N=4\n",
    "    embs = sentence_to_embeddings(sentence, N, device, dtype)\n",
    "\n",
    "    # Create a synthetic dataset if we want to do a batch approach\n",
    "    dataset = SyntheticTokenDataset(total_samples=200, seq_length=16, vocab_size=1000)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # We'll do a simple fixed number of epochs or steps\n",
    "    max_epochs = 3\n",
    "    steps_per_epoch = 20\n",
    "\n",
    "    # No gradients => we just do iterative PDE + embedding updates\n",
    "    step_count=0\n",
    "    for epoch in range(max_epochs):\n",
    "        for batch_idx, token_batch in enumerate(loader):\n",
    "            step_count+=1\n",
    "            # (1) PDE step with Heun-Euler\n",
    "            # PDE residual => measure\n",
    "            u_old = u.clone()\n",
    "            u_new = heun_euler_step(u_old, r_grid, alpha, dt)\n",
    "            u[:] = u_new  # update in place\n",
    "\n",
    "            # PDE Score = mean of residual^2\n",
    "            pde_score = (radial_diffusion_rhs(u, r_grid, alpha)**2).mean().item()\n",
    "\n",
    "            # (2) Update embeddings => negative distance logic\n",
    "            # We'll do a direct numeric approach\n",
    "            embs_old = embs.clone()\n",
    "            embs_new = update_embeddings(embs_old, dt, dist_scale=0.05)\n",
    "            embs[:] = embs_new  # in place\n",
    "\n",
    "            # Negative distance \"score\"\n",
    "            M = negative_distance_matrix(embs)\n",
    "            neg_score = M.mean().item()\n",
    "\n",
    "            # (3) Possibly adjust dt => dynamic\n",
    "            dt_old = dt\n",
    "            dt = dynamic_dt_update(dt, pde_score, abs(neg_score), dt_scale=0.02)\n",
    "\n",
    "            if step_count%5 ==0:\n",
    "                print(f\"Epoch={epoch}, step={step_count}, PDE_score={pde_score:.4e}, \"\n",
    "                      f\"negdist_score={neg_score:.4e}, dt={dt_old:.3e} -> {dt:.3e}\")\n",
    "\n",
    "            if step_count>= steps_per_epoch:\n",
    "                break\n",
    "        if step_count>= steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    # Final output\n",
    "    print(\"\\n[INFO] Final wavefunction sample:\", u[::10].cpu().numpy())\n",
    "    print(\"[INFO] Negative distance matrix sample:\\n\", M[:3,:3].cpu().numpy())\n",
    "\n",
    "    # Show a quick \"reconstructed\" sentence\n",
    "    # (Force 'I'->'like' start => then pick next by max neg dist)\n",
    "    def reconstruct_sentence(sentence, negdist_mat):\n",
    "        D_ = len(sentence)\n",
    "        if D_<2: return sentence[:]\n",
    "        visited = [False]*D_\n",
    "        order=[]\n",
    "        visited[0]=True\n",
    "        visited[1]=True\n",
    "        order.append(sentence[0])\n",
    "        order.append(sentence[1])\n",
    "        current=1\n",
    "        for _ in range(D_-2):\n",
    "            row = negdist_mat[current]\n",
    "            cand_j=-1\n",
    "            cand_val=-1e9\n",
    "            for j in range(D_):\n",
    "                val_j = row[j].item()\n",
    "                if (not visited[j]) and (val_j>cand_val):\n",
    "                    cand_val= val_j\n",
    "                    cand_j=j\n",
    "            visited[cand_j]=True\n",
    "            current=cand_j\n",
    "            order.append(sentence[current])\n",
    "        return order\n",
    "\n",
    "    reorder = reconstruct_sentence(sentence, M)\n",
    "    print(\"\\nReconstructed sentence (no grad approach):\", reorder)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Optional: torch.profiler\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available():\n",
    "        activities.append(torch.profiler.ProfilerActivity.CUDA)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=activities,\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=False\n",
    "    ) as prof:\n",
    "        main()\n",
    "\n",
    "    print(\"\\n== Profiler Summary (Top 10 by CPU time) ==\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\n== Profiler Summary (Top 10 by self CUDA time) ==\")\n",
    "        print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
